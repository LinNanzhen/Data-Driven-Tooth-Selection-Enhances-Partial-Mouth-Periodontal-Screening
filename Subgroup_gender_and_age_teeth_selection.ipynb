{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97819313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import contextlib\n",
    "import io\n",
    "from scipy import interp\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "           \n",
    "# ================== Library Imports ==================\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    SKOPT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKOPT_AVAILABLE = False\n",
    "    print(\"Warning: scikit-optimize is not installed. Bayesian optimization will not be available.\")\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"Warning: shap is not installed. Model interpretation will not be available.\")\n",
    "\n",
    "try:\n",
    "    import xlsxwriter\n",
    "    XLSXWRITER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XLSXWRITER_AVAILABLE = False\n",
    "    print(\"Warning: xlsxwriter is not installed. Saving SHAP values to Excel will not be available.\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import with proper error handling\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================== Configuration ==================\n",
    "class Config:\n",
    "    BASE_DIR = Path('.')\n",
    "    DATA_PATH = BASE_DIR / \"cleaned_data.csv\"\n",
    "    OUTPUT_DIR = BASE_DIR / \"Gender_and_age_analysis\"\n",
    "    TARGET_COLUMN = 'perio_label_cdc'\n",
    "    AGE_COLUMN = 'RIDAGEYR'\n",
    "    GENDER_COLUMN = 'RIAGENDR' \n",
    "    WEIGHT_COLUMN = 'WTMEC2YR'\n",
    "    MIN_AGE = 35\n",
    "    MIN_SAMPLES_PER_GROUP = 100\n",
    "    RANDOM_SEEDS = [42, 123, 456, 999, 2025]\n",
    "    CV_FOLDS = 5\n",
    "    INTERPROXIMAL_SITES = {'D', 'S', 'P', 'A'}\n",
    "    CLASS_LABELS = {0: 'Healthy', 1: 'Other', 2: 'Severe'}\n",
    "\n",
    "    ANALYSIS_GROUPS = {\n",
    "        '35-44_Male':   {'age_min': 35, 'age_max': 44, 'gender': 1},\n",
    "        '35-44_Female': {'age_min': 35, 'age_max': 44, 'gender': 2},\n",
    "        '45-54_Male':   {'age_min': 45, 'age_max': 54, 'gender': 1},\n",
    "        '45-54_Female': {'age_min': 45, 'age_max': 54, 'gender': 2},\n",
    "        '55-64_Male':   {'age_min': 55, 'age_max': 64, 'gender': 1},\n",
    "        '55-64_Female': {'age_min': 55, 'age_max': 64, 'gender': 2},\n",
    "        '65-74_Male':   {'age_min': 65, 'age_max': 74, 'gender': 1},\n",
    "        '65-74_Female': {'age_min': 65, 'age_max': 74, 'gender': 2},\n",
    "        '75+_Male':     {'age_min': 75, 'age_max': 999, 'gender': 1},\n",
    "        '75+_Female':   {'age_min': 75, 'age_max': 999, 'gender': 2},\n",
    "    }\n",
    "\n",
    "    TOP_FEATURES_COUNT = 10\n",
    "    SHAP_SAMPLE_SIZE = 1000\n",
    "    SHAP_PLOT_MAX_DISPLAY = 20\n",
    "\n",
    "    CPI_RAMFJORD_TOOTH_NUMBERS = {\n",
    "        'Ramfjord': ['16', '14', '21', '24', '26', '36', '34', '41', '44', '46'],\n",
    "        'CPI': [ '11', '16', '17', '26', '27', '31', '36', '37', '46', '47']\n",
    "    }\n",
    "    \n",
    "    NHANES_TO_FDI_MAPPING = {\n",
    "        '01': '18', '02': '17', '03': '16', '04': '15', '05': '14', '06': '13', '07': '12', '08': '11',\n",
    "        '09': '21', '10': '22', '11': '23', '12': '24', '13': '25', '14': '26', '15': '27', '16': '28',\n",
    "        '17': '38', '18': '37', '19': '36', '20': '35', '21': '34', '22': '33', '23': '32', '24': '31',\n",
    "        '25': '41', '26': '42', '27': '43', '28': '44', '29': '45', '30': '46', '31': '47', '32': '48'\n",
    "    }\n",
    "\n",
    "    # Optimized Bayesian Hyperparameter Space\n",
    "    BAYESIAN_HYPERPARAMETER_SPACES = {\n",
    "        'XGBoost': {\n",
    "            'n_estimators': Integer(900, 1200),\n",
    "            'max_depth': Integer(2, 4),\n",
    "            'learning_rate': Real(0.03, 0.2, 'log-uniform'),\n",
    "            'subsample': Real(0.6, 1.0, 'uniform'),\n",
    "            'colsample_bytree': Real(0.75, 0.95, 'uniform'),\n",
    "            'gamma': Real(0.0, 1.0, 'uniform'),\n",
    "            'reg_alpha': Real(1e-4, 1.0, 'log-uniform'),\n",
    "            'reg_lambda': Real(1e-5, 1e-1, 'log-uniform'),\n",
    "            'min_child_weight': Integer(5, 12)\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'n_estimators': Integer(800, 1500),\n",
    "            'max_depth': Integer(2, 4),\n",
    "            'num_leaves': Integer(50, 200),\n",
    "            'learning_rate': Real(0.005, 0.2, 'log-uniform'),\n",
    "            'subsample': Real(0.6, 1.0, 'uniform'),\n",
    "            'colsample_bytree': Real(0.5, 0.85, 'uniform'),\n",
    "            'reg_alpha': Real(1e-5, 1e-2, 'log-uniform'),\n",
    "            'reg_lambda': Real(1.0, 20.0, 'log-uniform'),\n",
    "            'min_child_samples': Integer(5, 30)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    BAYESIAN_N_ITER = 30\n",
    "\n",
    "Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ================== Utility Functions ==================\n",
    "def create_model(model_name, random_state=42, **kwargs):\n",
    "    \"\"\"Creates a model instance based on the model name.\"\"\"\n",
    "    if model_name == 'XGBoost':\n",
    "        if not XGB_AVAILABLE:\n",
    "            raise ImportError(\"XGBoost is not available\")\n",
    "        return xgb.XGBClassifier(\n",
    "            objective='multi:softprob', \n",
    "            random_state=random_state, \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='mlogloss', \n",
    "            n_jobs=1, \n",
    "            verbosity=0, \n",
    "            **kwargs\n",
    "        )\n",
    "    elif model_name == 'LightGBM':\n",
    "        if not LGB_AVAILABLE:\n",
    "            raise ImportError(\"LightGBM is not available\")\n",
    "        return lgb.LGBMClassifier(\n",
    "            objective='multiclass', \n",
    "            random_state=random_state, \n",
    "            n_jobs=-1, \n",
    "            class_weight= None, \n",
    "            verbose=-1,  # This suppresses LightGBM output\n",
    "            **kwargs\n",
    "        )\n",
    "    else: \n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "# ================== Data Processor ==================\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.pd_columns = []\n",
    "        self.cal_columns = []\n",
    "\n",
    "    def load_and_preprocess(self, filepath):\n",
    "        print(f\"Loading data from: {filepath}\")\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, low_memory=False)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Data file not found at {filepath}. Please ensure 'cleaned_data.csv' is in the correct directory.\")\n",
    "            # Create a dummy dataframe to avoid crashing the script\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        required_cols = [Config.AGE_COLUMN, Config.WEIGHT_COLUMN]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing required columns: {missing_cols}\")\n",
    "            for col in missing_cols:\n",
    "                if col == Config.WEIGHT_COLUMN: df[col] = 1.0\n",
    "                elif col == Config.AGE_COLUMN: df[col] = 40\n",
    "        \n",
    "        print(f\"Initial sample size: {len(df)}\")\n",
    "        df = df[df[Config.AGE_COLUMN] >= Config.MIN_AGE].copy()\n",
    "        print(f\"Sample size after age filtering (>= {Config.MIN_AGE}): {len(df)}\")\n",
    "        \n",
    "        initial_weighted_size = len(df)\n",
    "        df = df[(df[Config.WEIGHT_COLUMN].notna()) & (df[Config.WEIGHT_COLUMN] > 0)].copy()\n",
    "        print(f\"Sample size after weight filtering: {len(df)} (removed {initial_weighted_size - len(df)} records)\")\n",
    "        \n",
    "        df = self._identify_periodontal_features(df)\n",
    "        df = self._handle_missing_values(df)\n",
    "        df = self._apply_cdc_classification(df)\n",
    "        \n",
    "        print(\"\\nData preprocessing complete.\")\n",
    "        return df\n",
    "\n",
    "    def _identify_periodontal_features(self, df):\n",
    "        pd_pattern = r'^OHX\\d{2}PC[ADSP]$'\n",
    "        cal_pattern = r'^OHX\\d{2}LA[ADSP]$'\n",
    "        self.pd_columns = sorted([c for c in df.columns if re.search(pd_pattern, c)])\n",
    "        self.cal_columns = sorted([c for c in df.columns if re.search(cal_pattern, c)])\n",
    "        for col in self.pd_columns + self.cal_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        return df\n",
    "\n",
    "    def _handle_missing_values(self, df):\n",
    "        for col in self.pd_columns + self.cal_columns:\n",
    "            df[col] = df[col].fillna(np.nan)\n",
    "        return df\n",
    "\n",
    "    def _apply_cdc_classification(self, df):\n",
    "        pd_cols = self.pd_columns  # PD （OHX##PC[ADSP]）\n",
    "        cal_cols = self.cal_columns # CAL （OHX##LA[ADSP]）\n",
    "        impute_cols = pd_cols + cal_cols\n",
    "        df_imputed = df[impute_cols].copy()\n",
    "        df_imputed.replace(99, np.nan, inplace=True)\n",
    "\n",
    "        #  Construct the Max of Adjacent Sites per Tooth (One Table Each for PPD and CAL)\n",
    "        tooth_prefixes = sorted({c[:5] for c in impute_cols}) \n",
    "        tooth_pd_cols = {tp: [] for tp in tooth_prefixes}\n",
    "        tooth_cal_cols = {tp: [] for tp in tooth_prefixes}\n",
    "\n",
    "        for col in pd_cols:\n",
    "            if col[-1] in Config.INTERPROXIMAL_SITES:\n",
    "                tooth_pd_cols[col[:5]].append(col)\n",
    "        for col in cal_cols:\n",
    "            if col[-1] in Config.INTERPROXIMAL_SITES:\n",
    "                tooth_cal_cols[col[:5]].append(col)\n",
    "\n",
    "        tooth_pd_max = {}\n",
    "        tooth_cal_max = {}\n",
    "        for tp in tooth_prefixes:\n",
    "            cols_pd = tooth_pd_cols[tp]\n",
    "            cols_cal = tooth_cal_cols[tp]\n",
    "            tooth_pd_max[tp]  = df_imputed[cols_pd].max(axis=1) if cols_pd else pd.Series(np.nan, index=df.index)\n",
    "            tooth_cal_max[tp] = df_imputed[cols_cal].max(axis=1) if cols_cal else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "        tooth_pd_max_df  = pd.DataFrame(tooth_pd_max)\n",
    "        tooth_cal_max_df = pd.DataFrame(tooth_cal_max)\n",
    "\n",
    "        # Count the 'Number of Teeth Meeting the Threshold' (Different Tooth Constraints)\n",
    "        def count_teeth_ge(mat: pd.DataFrame, thr: float) -> pd.Series:\n",
    "            return (mat >= thr).sum(axis=1)\n",
    "\n",
    "        n_CAL3 = count_teeth_ge(tooth_cal_max_df, 3.0)\n",
    "        n_CAL4 = count_teeth_ge(tooth_cal_max_df, 4.0)\n",
    "        n_CAL6 = count_teeth_ge(tooth_cal_max_df, 6.0)\n",
    "        n_PPD4 = count_teeth_ge(tooth_pd_max_df,  4.0)\n",
    "        n_PPD5 = count_teeth_ge(tooth_pd_max_df,  5.0)\n",
    "\n",
    "        # Classification Logic (Prioritize Severe Assessment, Followed by Moderate, Then Mild)\n",
    "        severe = (n_CAL6 >= 2) & (n_PPD5 >= 1) \n",
    "\n",
    "        other = ((~severe) & (n_CAL3 >= 2) & ((n_PPD4 >= 2) | (n_PPD5 >= 1)))\n",
    "        \n",
    "        label = np.where(severe, 2, np.where(other, 1, 0)).astype(int)\n",
    "\n",
    "        df[Config.TARGET_COLUMN] = label\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _create_max_features(self, df, column_list, feature_type):\n",
    "        if not column_list:\n",
    "            return pd.DataFrame(index=df.index)\n",
    "        \n",
    "        data_for_agg = df[column_list].replace(99, np.nan)\n",
    "        grouped_cols = defaultdict(list)\n",
    "        for col in column_list:\n",
    "            prefix = col[:-1]\n",
    "            grouped_cols[prefix].append(col)\n",
    "        \n",
    "        feature_series_list = []\n",
    "        for prefix, columns in grouped_cols.items():\n",
    "            tooth_data = data_for_agg[columns]\n",
    "            max_values = tooth_data.max(axis=1).fillna(np.nan)\n",
    "            max_values.name = f'{prefix}_max'\n",
    "            feature_series_list.append(max_values)\n",
    "        \n",
    "        if feature_series_list:\n",
    "            X_max = pd.concat(feature_series_list, axis=1)\n",
    "        else:\n",
    "            X_max = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        return X_max\n",
    "\n",
    "    def get_combined_feature_set(self, df):\n",
    "        print(\"Creating periodontal depth features...\")\n",
    "        X_pd_max = self._create_max_features(df, self.pd_columns, \"PD\")\n",
    "        print(f\"PD features shape: {X_pd_max.shape}\")\n",
    "        \n",
    "        print(\"Creating clinical attachment level features...\")\n",
    "        X_cal_max = self._create_max_features(df, self.cal_columns, \"CAL\")\n",
    "        print(f\"CAL features shape: {X_cal_max.shape}\")\n",
    "        \n",
    "        # Combine features using explicit column handling\n",
    "        print(\"Combining feature sets...\")\n",
    "        if X_pd_max.empty and X_cal_max.empty:\n",
    "            X_combined_max = pd.DataFrame(index=df.index)\n",
    "        elif X_pd_max.empty:\n",
    "            X_combined_max = X_cal_max.copy()\n",
    "        elif X_cal_max.empty:\n",
    "            X_combined_max = X_pd_max.copy()\n",
    "        else:\n",
    "            # Ensure both dataframes have same index\n",
    "            X_pd_max = X_pd_max.reindex(df.index)\n",
    "            X_cal_max = X_cal_max.reindex(df.index)\n",
    "            X_combined_max = pd.concat([X_pd_max, X_cal_max], axis=1)\n",
    "        \n",
    "        print(f\"Combined features shape: {X_combined_max.shape}\")\n",
    "        \n",
    "        y = df[Config.TARGET_COLUMN]\n",
    "        sample_weights = df[Config.WEIGHT_COLUMN] if Config.WEIGHT_COLUMN in df.columns else None\n",
    "        return X_combined_max, y, sample_weights\n",
    "\n",
    "    def nhanes_to_fdi(self, nhanes_tooth):\n",
    "        return Config.NHANES_TO_FDI_MAPPING.get(nhanes_tooth, nhanes_tooth)\n",
    "\n",
    "    def extract_tooth_numbers_from_features(self, feature_list):\n",
    "        tooth_numbers_fdi = set()\n",
    "        for feature in feature_list:\n",
    "            if 'OHX' in feature and ('PC_max' in feature or 'LA_max' in feature):\n",
    "                tooth_num = feature.replace('OHX', '').replace('PC_max', '').replace('LA_max', '')\n",
    "                if tooth_num.isdigit():\n",
    "                    fdi_num = self.nhanes_to_fdi(tooth_num)\n",
    "                    tooth_numbers_fdi.add(fdi_num)\n",
    "        return sorted(list(tooth_numbers_fdi))\n",
    "\n",
    "# ================== Bayesian Hyperparameter Tuning ==================\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, cv_folds=5, random_state=42):\n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def tune_model_bayes(self, model_name, base_model, X, y, sample_weight, param_space, scoring='roc_auc_ovr'):\n",
    "        print(f\"  Tuning hyperparameters for {model_name} with Bayesian Optimization...\")\n",
    "        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        bayes_search = BayesSearchCV(\n",
    "            base_model, param_space, n_iter=Config.BAYESIAN_N_ITER, \n",
    "            cv=cv, scoring=scoring, n_jobs=-1, random_state=self.random_state, verbose=0\n",
    "        )\n",
    "        \n",
    "        fit_params = {}\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            class_sample_weights = compute_sample_weight(\n",
    "                class_weight='balanced', \n",
    "                y=y\n",
    "            )\n",
    "            combined_weights = sample_weight * class_sample_weights\n",
    "            fit_params['sample_weight'] = combined_weights\n",
    "        else:\n",
    "            class_sample_weights = compute_sample_weight(\n",
    "                class_weight='balanced', \n",
    "                y=y\n",
    "            )\n",
    "            fit_params['sample_weight'] = class_sample_weights\n",
    "            \n",
    "        print(f\"    Applied class balancing during hyperparameter tuning for {model_name}\")\n",
    "\n",
    "        bayes_search.fit(X, y, **fit_params)\n",
    "            \n",
    "        tuned_model = base_model.set_params(**bayes_search.best_params_)\n",
    "        print(f\"    Best params: {bayes_search.best_params_}\")\n",
    "        print(f\"    Best score: {bayes_search.best_score_:.4f}\")\n",
    "        gc.collect()\n",
    "        return tuned_model, bayes_search.best_params_\n",
    "\n",
    "# ================== Multi-Seed Stability Analyzer ==================\n",
    "class MultiSeedStabilityAnalyzer:\n",
    "    def __init__(self, data_processor, random_seeds=Config.RANDOM_SEEDS):\n",
    "        self.data_processor = data_processor\n",
    "        self.random_seeds = random_seeds\n",
    "        self.stability_results = {}\n",
    "        self.all_run_shap_values = defaultdict(list)\n",
    "\n",
    "    def analyze_top_teeth_stability(self, X_train, y_train, sample_weights, model_name, best_params, current_output_dir):\n",
    "\n",
    "        import contextlib, io, sys\n",
    "        print(f\"\\n--- Analyzing Feature Stability for {model_name} (Robust Multi-Run SHAP) ---\")\n",
    "        \n",
    "        if not SHAP_AVAILABLE:\n",
    "            print(\"SHAP is not available. Skipping stability analysis.\")\n",
    "            return {}\n",
    "\n",
    "        self.all_run_shap_values.clear()\n",
    "\n",
    "        print(f\"Creating a fixed background sample of size {Config.SHAP_SAMPLE_SIZE} for SHAP analysis.\")\n",
    "        X_sample = X_train.sample(min(len(X_train), Config.SHAP_SAMPLE_SIZE), random_state=42)\n",
    "\n",
    "        total_runs = len(self.random_seeds) * Config.CV_FOLDS\n",
    "        run_count = 0\n",
    "\n",
    "        for seed_idx, seed in enumerate(self.random_seeds):\n",
    "            print(f\"\\n  Processing Seed {seed_idx + 1}/{len(self.random_seeds)}: {seed}\")\n",
    "            cv = StratifiedKFold(n_splits=Config.CV_FOLDS, shuffle=True, random_state=seed)\n",
    "            \n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "                run_count += 1\n",
    "                print(f\"    Fold {fold_idx + 1}/{Config.CV_FOLDS} (Overall Run {run_count}/{total_runs})\")\n",
    "                X_train_sub, _ = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_train_sub, _ = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                sw_train_sub = sample_weights.iloc[train_idx] if sample_weights is not None else None\n",
    "                \n",
    "                model = create_model(model_name, random_state=seed)\n",
    "                model.set_params(**best_params)\n",
    "                \n",
    "                fit_params = {}\n",
    "                class_sample_weights = compute_sample_weight('balanced', y=y_train_sub)\n",
    "                fit_params['sample_weight'] = sw_train_sub * class_sample_weights if sw_train_sub is not None else class_sample_weights\n",
    "                model.set_params(verbosity=0)\n",
    "                \n",
    "                with contextlib.redirect_stdout(io.StringIO()):\n",
    "                    model.fit(X_train_sub, y_train_sub, **fit_params)\n",
    "                \n",
    "                try:\n",
    "                    explainer = shap.TreeExplainer(model, model_output=\"probability\", feature_perturbation=\"interventional\")\n",
    "                except Exception:\n",
    "                    explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "                if isinstance(shap_values, list):\n",
    "                    mean_shap = np.mean([np.abs(sv).mean(axis=0) for sv in shap_values], axis=0)\n",
    "                else:\n",
    "                    mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "                for feature_name, shap_val in zip(X_train.columns, mean_shap):\n",
    "                    self.all_run_shap_values[feature_name].append(shap_val)\n",
    "                \n",
    "                del model, explainer, shap_values\n",
    "                gc.collect()\n",
    "        \n",
    "        stability_analysis = self._calculate_and_visualize_stability_results(model_name, current_output_dir)\n",
    "        self.stability_results[model_name] = stability_analysis\n",
    "        return stability_analysis\n",
    "\n",
    "    def _calculate_and_visualize_stability_results(self, model_name, current_output_dir):\n",
    "\n",
    "        print(f\"\\n  Aggregating and analyzing SHAP results for {model_name}...\")\n",
    "        \n",
    "        feature_stability_data = [{'feature': f, 'mean_shap': np.mean(s), 'std_shap': np.std(s)} \n",
    "                                  for f, s in self.all_run_shap_values.items()]\n",
    "        feature_df = pd.DataFrame(feature_stability_data).sort_values('mean_shap', ascending=False)\n",
    "\n",
    "        detailed_importance_data = []\n",
    "        for _, row in feature_df.iterrows():\n",
    "            match = re.search(r'OHX(\\d{2})(PC|LA)_max', row['feature'])\n",
    "            if match:\n",
    "                nhanes_num = match.group(1)\n",
    "                measurement_type = 'PD' if match.group(2) == 'PC' else 'CAL'\n",
    "                fdi_num = self.data_processor.nhanes_to_fdi(nhanes_num)\n",
    "                \n",
    "                detailed_importance_data.append({\n",
    "                    'tooth_fdi': fdi_num,\n",
    "                    'measurement_type': measurement_type,\n",
    "                    'feature': row['feature'],\n",
    "                    'mean_shap': row['mean_shap'],\n",
    "                    'shap_error': row['std_shap']\n",
    "                })\n",
    "        detailed_importance_df = pd.DataFrame(detailed_importance_data)\n",
    "\n",
    "        tooth_df_aggregated = detailed_importance_df.groupby('tooth_fdi').agg(\n",
    "            total_mean_shap=('mean_shap', 'sum'),\n",
    "            shap_error=('shap_error', 'mean') \n",
    "        ).sort_values('total_mean_shap', ascending=False).reset_index()\n",
    "\n",
    "        print(\"\\n--- Full Tooth Importance Ranking (Aggregated) ---\")\n",
    "        print(tooth_df_aggregated.to_string(index=False))\n",
    "\n",
    "        detailed_importance_df.to_csv(current_output_dir / f\"{model_name}_detailed_feature_importance.csv\", index=False)\n",
    "        tooth_df_aggregated.to_csv(current_output_dir / f\"{model_name}_aggregated_tooth_importance.csv\", index=False)\n",
    "        print(f\"\\nSaved detailed and aggregated importance to: {current_output_dir}\")\n",
    "\n",
    "        self.visualize_detailed_importance(detailed_importance_df, tooth_df_aggregated, model_name, current_output_dir)\n",
    "\n",
    "        analysis = {\n",
    "            'tooth_level_importance': tooth_df_aggregated, \n",
    "            'feature_level_stability': feature_df,\n",
    "            'detailed_importance': detailed_importance_df \n",
    "        }\n",
    "        return analysis\n",
    "\n",
    "    def visualize_detailed_importance(self, detailed_df, aggregated_df, model_name, current_output_dir):\n",
    "        \n",
    "        print(f\"Generating detailed visualizations for {model_name}...\")\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "        top_teeth = aggregated_df.head(10)['tooth_fdi'].tolist()\n",
    "        plot_data = detailed_df[detailed_df['tooth_fdi'].isin(top_teeth)]\n",
    "\n",
    "        pivot_df = plot_data.pivot_table(index='tooth_fdi', columns='measurement_type', values='mean_shap').fillna(0)\n",
    "        pivot_df = pivot_df.reindex(top_teeth)  \n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(18, 9))\n",
    "        bar_width = 0.35\n",
    "        index = np.arange(len(pivot_df.index))\n",
    "\n",
    "        err_df = plot_data.pivot_table(index='tooth_fdi',\n",
    "                                    columns='measurement_type',\n",
    "                                    values='shap_error',\n",
    "                                    aggfunc='mean').fillna(0).reindex(top_teeth)\n",
    "\n",
    "        pd_means = pivot_df['PD']\n",
    "        pd_std = err_df['PD']\n",
    "        pd_lower = np.minimum(pd_std, pd_means)\n",
    "        pd_upper = pd_std\n",
    "\n",
    "        cal_means = pivot_df['CAL']\n",
    "        cal_std = err_df['CAL']\n",
    "        cal_lower = np.minimum(cal_std, cal_means)\n",
    "        cal_upper = cal_std\n",
    "\n",
    "        bars1 = ax.bar(\n",
    "            index - bar_width / 2, pd_means, bar_width,\n",
    "            yerr=[pd_lower.to_numpy(), pd_upper.to_numpy()],\n",
    "            capsize=4, label='PD', color='royalblue'\n",
    "        )\n",
    "        bars2 = ax.bar(\n",
    "            index + bar_width / 2, cal_means, bar_width,\n",
    "            yerr=[cal_lower.to_numpy(), cal_upper.to_numpy()],\n",
    "            capsize=4, label='CAL', color='skyblue'\n",
    "        )\n",
    "\n",
    "        ax.set_ylabel('Robust Mean(|SHAP|) Value', fontsize=14)\n",
    "        ax.set_xlabel('Tooth (FDI Notation)', fontsize=14)\n",
    "        ax.set_title(f'PD vs. CAL Importance for Top 10 Teeth - {model_name}', fontsize=16, pad=20)\n",
    "        ax.set_xticks(index)\n",
    "        ax.set_xticklabels(pivot_df.index, rotation=45, ha=\"right\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "\n",
    "        output_path_paired = current_output_dir / f\"{model_name}_paired_importance_barchart.png\"\n",
    "        plt.savefig(output_path_paired, dpi=800)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved paired bar chart to: {output_path_paired}\")\n",
    "\n",
    "        FDI_LAYOUT = {\n",
    "            '18': (0, 0), '17': (0, 1), '16': (0, 2), '15': (0, 3), '14': (0, 4), '13': (0, 5), '12': (0, 6), '11': (0, 7),\n",
    "            '21': (0, 8), '22': (0, 9), '23': (0,10), '24': (0,11), '25': (0,12), '26': (0,13), '27': (0,14), '28': (0,15),\n",
    "            '48': (1, 0), '47': (1, 1), '46': (1, 2), '45': (1, 3), '44': (1, 4), '43': (1, 5), '42': (1, 6), '41': (1, 7),\n",
    "            '31': (1, 8), '32': (1, 9), '33': (1,10), '34': (1,11), '35': (1,12), '36': (1,13), '37': (1,14), '38': (1,15)\n",
    "        }\n",
    "\n",
    "        heatmap_data = np.full((2, 16), np.nan)\n",
    "        importance_dict = aggregated_df.set_index('tooth_fdi')['total_mean_shap'].to_dict()\n",
    "        for fdi, pos in FDI_LAYOUT.items():\n",
    "            heatmap_data[pos] = importance_dict.get(fdi, np.nan)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 4))\n",
    "        im = ax.imshow(heatmap_data, cmap='Reds', interpolation='nearest', aspect='auto')\n",
    "\n",
    "        cbar = fig.colorbar(im, ax=ax, fraction=0.02, pad=0.04)\n",
    "        cbar.set_label('Aggregated Robust Mean(|SHAP|)', rotation=270, labelpad=15)\n",
    "\n",
    "        max_val = np.nanmax(heatmap_data)\n",
    "        for fdi, pos in FDI_LAYOUT.items():\n",
    "            row, col = pos\n",
    "            val = heatmap_data[row, col]\n",
    "            if np.isnan(val):\n",
    "                continue\n",
    "            text = f\"{fdi}\\n{val:.2f}\"\n",
    "            text_color = 'white' if val > (max_val / 2) else 'black'\n",
    "            ax.text(col, row, text, ha='center', va='center',\n",
    "                    color=text_color, weight='bold', fontsize=9)\n",
    "\n",
    "        ax.set_title(f'Aggregated Tooth Importance Heatmap for {model_name}', fontsize=16, pad=20)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.tight_layout()\n",
    "\n",
    "        output_path_heatmap = current_output_dir / f\"{model_name}_aggregated_importance_heatmap.png\"\n",
    "        plt.savefig(output_path_heatmap, dpi=800)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved heatmap to: {output_path_heatmap}\")\n",
    "\n",
    "# ================== Model Evaluation Pipeline ==================\n",
    "class ModelPipeline:\n",
    "    def __init__(self, random_seeds=Config.RANDOM_SEEDS, cv_folds=Config.CV_FOLDS):\n",
    "        self.random_seeds = random_seeds\n",
    "        self.cv_folds = cv_folds\n",
    "        self.tuner = HyperparameterTuner(cv_folds=cv_folds)\n",
    "        self.best_params_cache = {}\n",
    "\n",
    "    def _calculate_weighted_metrics(self, y_true, y_pred, y_pred_proba, sample_weight=None):\n",
    "        auc_macro = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro', sample_weight=sample_weight)\n",
    "        accuracy = accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro', sample_weight=sample_weight)\n",
    "        qwk = cohen_kappa_score(y_true, y_pred, weights='quadratic', sample_weight=sample_weight)\n",
    "        return {'auc_macro': auc_macro, 'accuracy': accuracy, 'f1_macro': f1_macro, 'qwk': qwk}\n",
    "\n",
    "    def tune_hyperparameters_once(self, model_name, X, y, sample_weight, feature_set_name='Combined'):\n",
    "            \n",
    "            param_key = f\"{model_name}_{feature_set_name}\"\n",
    "            if param_key in self.best_params_cache:\n",
    "                print(f\"  Using cached hyperparameters for {param_key}\")\n",
    "                return self.best_params_cache[param_key]\n",
    "            \n",
    "            if model_name in Config.BAYESIAN_HYPERPARAMETER_SPACES and SKOPT_AVAILABLE:\n",
    "                base_model = create_model(model_name)\n",
    "                _, best_params = self.tuner.tune_model_bayes(\n",
    "                    model_name, base_model, X, y, sample_weight, \n",
    "                    Config.BAYESIAN_HYPERPARAMETER_SPACES[model_name]\n",
    "                )\n",
    "                self.best_params_cache[param_key] = best_params\n",
    "                del base_model\n",
    "                gc.collect()\n",
    "            else:\n",
    "                print(f\"  Skipping Bayesian tuning for {model_name} (scikit-optimize not available or not configured). Using default parameters.\")\n",
    "                best_params = {}\n",
    "                self.best_params_cache[param_key] = best_params\n",
    "            return best_params\n",
    "\n",
    "    def evaluate_feature_sets_multiseed(self, X, y, sample_weights, model_name, best_params, stability_analyzer, current_output_dir):\n",
    "        print(f\"\\n{'='*50}\\nMULTI-SEED EVALUATION: {model_name}\\n{'='*50}\")\n",
    "        all_results = []\n",
    "\n",
    "        stability_results = stability_analyzer.analyze_top_teeth_stability(\n",
    "            X, y, sample_weights, model_name, best_params, current_output_dir\n",
    "        )\n",
    "        \n",
    "        if 'tooth_level_importance' in stability_results:\n",
    "            consensus_df = self._create_consensus_feature_sets(X, stability_results, model_name).get('SHAP_Consensus', pd.DataFrame())\n",
    "        else:\n",
    "            consensus_df = pd.DataFrame()\n",
    "\n",
    "        feature_sets = {\n",
    "            'Combined': X,\n",
    "            'CPI': self._get_clinical_index_features(X, 'CPI'),\n",
    "            'Ramfjord': self._get_clinical_index_features(X, 'Ramfjord'),\n",
    "            f'SHAP_Consensus_{model_name}': consensus_df  \n",
    "        }\n",
    "\n",
    "        roc_data = {}\n",
    "        for fs_name, X_fs in feature_sets.items():\n",
    "            if X_fs.empty:\n",
    "                print(f\"Skipping {fs_name} (no features)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n>>> Tuning {model_name} / {fs_name} ...\")\n",
    "            cache_name = 'SHAP_Consensus' if fs_name.startswith('SHAP_Consensus_') else fs_name\n",
    "            best_params_fs = self.tune_hyperparameters_once(\n",
    "                model_name, X_fs, y, sample_weights,\n",
    "                feature_set_name=cache_name\n",
    "            )\n",
    "\n",
    "            print(f\"\\nEvaluating {fs_name} ({X_fs.shape[1]} features)...\")\n",
    "            metrics_across_seeds, roc_curves = self._evaluate_with_multiseed_cv(\n",
    "                X_fs, y, sample_weights, model_name, best_params_fs\n",
    "            )\n",
    "\n",
    "            for metric, vals in metrics_across_seeds.items():\n",
    "                all_results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Feature_Set': fs_name,  \n",
    "                    'Metric': metric,\n",
    "                    'Mean': np.mean(vals),\n",
    "                    'Std': np.std(vals),\n",
    "                    'N_Features': X_fs.shape[1]\n",
    "                })\n",
    "\n",
    "            roc_data[fs_name] = roc_curves\n",
    "\n",
    "        n_feature_sets = len([fs for fs in feature_sets.values() if not fs.empty])\n",
    "        \n",
    "        group_name = current_output_dir.name          \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for idx, (cls_key, class_label) in enumerate(Config.CLASS_LABELS.items()):\n",
    "            ax = axes[idx]\n",
    "\n",
    "            colors = ['blue','red','green','orange','purple']\n",
    "            color_idx = 0\n",
    "            for fs_name, curves in roc_data.items():\n",
    "                fpr_tpr_list = curves[class_label]\n",
    "                all_fpr = np.unique(np.concatenate([fpr for fpr,_ in fpr_tpr_list]))\n",
    "                mean_tpr = np.zeros_like(all_fpr)\n",
    "                for fpr,tpr in fpr_tpr_list:\n",
    "                    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
    "                mean_tpr /= len(fpr_tpr_list)\n",
    "                mean_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "                display_name = fs_name\n",
    "                if fs_name.startswith('SHAP_Consensus_'):\n",
    "                    display_name = f\"SHAP_{model_name}\"\n",
    "\n",
    "                ax.plot(all_fpr, mean_tpr,\n",
    "                        label=f\"{display_name} (AUC={mean_auc:.2f})\",\n",
    "                        linewidth=2,\n",
    "                        color=colors[color_idx % len(colors)])\n",
    "                color_idx += 1\n",
    "\n",
    "            ax.plot([0,1],[0,1],'--',color='gray',alpha=.5)\n",
    "            ax.set_title(f\"{class_label} – {model_name} – {group_name}\")\n",
    "            ax.set_xlabel(\"False Positive Rate\")\n",
    "            ax.set_ylabel(\"True Positive Rate\")\n",
    "            ax.legend(loc=\"lower right\",fontsize='small')\n",
    "\n",
    "        fig.suptitle(f\"{model_name} – {group_name} – Cross-validation ROC-AUC (training set)\", fontsize=16)\n",
    "        fig.tight_layout(rect=[0,0,1,0.96])\n",
    "\n",
    "        roc_out_path = current_output_dir / f\"roc_group_{model_name}_train.png\"\n",
    "        fig.savefig(roc_out_path, dpi=800, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved grouped ROC figure: {roc_out_path}\")\n",
    "\n",
    "        return all_results, stability_results\n",
    "\n",
    "    def _create_consensus_feature_sets(self, X_full, stability_results, model_name):\n",
    "        feature_sets = {}\n",
    "        \n",
    "        if 'tooth_level_importance' in stability_results:\n",
    "            tooth_df = stability_results['tooth_level_importance']\n",
    "            top_teeth_fdi = tooth_df.head(Config.TOP_FEATURES_COUNT)['tooth_fdi'].tolist()\n",
    "            \n",
    "            if top_teeth_fdi:\n",
    "                shap_features = []\n",
    "                for tooth_fdi in top_teeth_fdi:\n",
    "                    nhanes_tooth = next((nhanes for nhanes, fdi in Config.NHANES_TO_FDI_MAPPING.items() if fdi == tooth_fdi), None)\n",
    "                    if nhanes_tooth:\n",
    "                        for p_type in ['PC', 'LA']:\n",
    "                            feature = f'OHX{nhanes_tooth}{p_type}_max'\n",
    "                            if feature in X_full.columns:\n",
    "                                shap_features.append(feature)\n",
    "                \n",
    "                if shap_features:\n",
    "                    feature_sets['SHAP_Consensus'] = X_full[shap_features]\n",
    "                    print(f\"  Created {model_name} SHAP consensus set from robust ranking with teeth: {sorted(top_teeth_fdi)}\")\n",
    "\n",
    "        feature_sets['CPI_Reference'] = self._get_clinical_index_features(X_full, 'CPI')\n",
    "        feature_sets['Ramfjord_Reference'] = self._get_clinical_index_features(X_full, 'Ramfjord')\n",
    "        \n",
    "        return feature_sets\n",
    "\n",
    "    def _get_clinical_index_features(self, X_combined, method_name):\n",
    "        fdi_numbers = Config.CPI_RAMFJORD_TOOTH_NUMBERS[method_name]\n",
    "        selected_features = []\n",
    "        for fdi_num in fdi_numbers:\n",
    "            nhanes_num = next((nhanes for nhanes, fdi in Config.NHANES_TO_FDI_MAPPING.items() if fdi == fdi_num), None)\n",
    "            if nhanes_num:\n",
    "                for p_type in ['PC', 'LA']:\n",
    "                    feature = f'OHX{nhanes_num}{p_type}_max'\n",
    "                    if feature in X_combined.columns: selected_features.append(feature)\n",
    "        \n",
    "        return X_combined[selected_features] if selected_features else pd.DataFrame()\n",
    "\n",
    "    def _evaluate_with_multiseed_cv(self, X, y, sample_weights, model_name, best_params):\n",
    "        all_metrics = defaultdict(list)\n",
    "        roc_curves = {cls: [] for cls in Config.CLASS_LABELS.values()}\n",
    "\n",
    "        classes = list(Config.CLASS_LABELS.keys())\n",
    "        y_bin_full = label_binarize(y, classes=classes)\n",
    "\n",
    "        for seed in self.random_seeds:\n",
    "            cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=seed)\n",
    "            for train_idx, val_idx in cv.split(X, y):\n",
    "                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                w_train = sample_weights.iloc[train_idx] if sample_weights is not None else None\n",
    "                w_val   = sample_weights.iloc[val_idx]   if sample_weights is not None else None\n",
    "\n",
    "                model = create_model(model_name, random_state=seed)\n",
    "                model.set_params(**best_params)\n",
    "\n",
    "                fit_params = {}\n",
    "                \n",
    "                class_sample_weights = compute_sample_weight(\n",
    "                    class_weight='balanced', \n",
    "                    y=y_train\n",
    "                )\n",
    "                \n",
    "                if w_train is not None:\n",
    "                    combined_weights = w_train * class_sample_weights\n",
    "                    fit_params['sample_weight'] = combined_weights\n",
    "                else:\n",
    "                    fit_params['sample_weight'] = class_sample_weights\n",
    "                \n",
    "                model.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "                y_score = model.predict_proba(X_val)\n",
    "                y_val_bin = y_bin_full[val_idx, :]\n",
    "\n",
    "                for i, class_name in enumerate(Config.CLASS_LABELS.values()):\n",
    "                    fpr_i, tpr_i, _ = roc_curve(\n",
    "                        y_val_bin[:, i],\n",
    "                        y_score[:, i],\n",
    "                        sample_weight=w_val\n",
    "                    )\n",
    "                    roc_curves[class_name].append((fpr_i, tpr_i))\n",
    "\n",
    "                metrics = self._calculate_weighted_metrics(\n",
    "                    y_val, model.predict(X_val), y_score, w_val\n",
    "                )\n",
    "                for name, v in metrics.items():\n",
    "                    all_metrics[name].append(v)\n",
    "\n",
    "        return all_metrics, roc_curves\n",
    "    \n",
    "    def evaluate_test_set(self, X_train, y_train, X_test, y_test, sw_train, sw_test, \n",
    "                        model_name, best_params, stability_results, output_dir):\n",
    "\n",
    "        print(f\"\\n--- Test Set Evaluation for {model_name} ---\")\n",
    "        \n",
    "        test_results = []\n",
    "        test_roc_data = {}\n",
    "        \n",
    "        classes = list(Config.CLASS_LABELS.keys())\n",
    "        y_test_bin = label_binarize(y_test, classes=classes)\n",
    "        \n",
    "        feature_sets = {\n",
    "            'Combined': X_train,\n",
    "            'CPI': self._get_clinical_index_features(X_train, 'CPI'),\n",
    "            'Ramfjord': self._get_clinical_index_features(X_train, 'Ramfjord'),\n",
    "        }\n",
    "        \n",
    "        if 'tooth_level_importance' in stability_results:\n",
    "            consensus_features = self._create_consensus_feature_sets(X_train, stability_results, model_name)\n",
    "            if 'SHAP_Consensus' in consensus_features and not consensus_features['SHAP_Consensus'].empty:\n",
    "                feature_sets[f'SHAP_Consensus_{model_name}'] = consensus_features['SHAP_Consensus']\n",
    "        \n",
    "        for fs_name, X_fs_train in feature_sets.items():\n",
    "            if X_fs_train.empty:\n",
    "                print(f\"Skipping {fs_name} (empty feature set)\")\n",
    "                continue\n",
    "                \n",
    "            X_fs_test = X_test[X_fs_train.columns]\n",
    "            \n",
    "            print(f\"Evaluating {fs_name} on test set ({X_fs_train.shape[1]} features)...\")\n",
    "            \n",
    "            model = create_model(model_name, random_state=42)\n",
    "            model.set_params(**best_params)\n",
    "            \n",
    "            fit_params = {}\n",
    "            class_sample_weights = compute_sample_weight('balanced', y=y_train)\n",
    "            fit_params['sample_weight'] = sw_train * class_sample_weights if sw_train is not None else class_sample_weights\n",
    "            \n",
    "            model.fit(X_fs_train, y_train, **fit_params)\n",
    "            \n",
    "            y_pred = model.predict(X_fs_test)\n",
    "            y_pred_proba = model.predict_proba(X_fs_test)\n",
    "            \n",
    "            metrics = self._calculate_weighted_metrics(y_test, y_pred, y_pred_proba, sw_test)\n",
    "            \n",
    "            for metric_name, score in metrics.items():\n",
    "                test_results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Feature_Set': fs_name,\n",
    "                    'Metric': metric_name,\n",
    "                    'Test_Score': score,\n",
    "                    'N_Features': X_fs_train.shape[1],\n",
    "                    'Test_Samples': len(X_test)\n",
    "                })\n",
    "            \n",
    "            test_roc_data[fs_name] = {}\n",
    "            for i, class_name in enumerate(Config.CLASS_LABELS.values()):\n",
    "                fpr, tpr, _ = roc_curve(\n",
    "                    y_test_bin[:, i],\n",
    "                    y_pred_proba[:, i],\n",
    "                    sample_weight=sw_test\n",
    "                )\n",
    "                test_roc_data[fs_name][class_name] = (fpr, tpr)\n",
    "            \n",
    "            print(f\"  {fs_name}: AUC={metrics['auc_macro']:.3f}, Acc={metrics['accuracy']:.3f}\")\n",
    "        \n",
    "        if test_roc_data:\n",
    "            self._plot_test_roc_curves(test_roc_data, model_name, output_dir)\n",
    "        \n",
    "        return test_results\n",
    "    \n",
    "    def _plot_test_roc_curves(self, test_roc_data, model_name, output_dir):\n",
    "        print(f\"Plotting test set ROC curves for {model_name}...\")\n",
    "        \n",
    "        group_name = output_dir.name\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "        \n",
    "        for idx, (cls_key, class_label) in enumerate(Config.CLASS_LABELS.items()):\n",
    "            ax = axes[idx]\n",
    "            color_idx = 0\n",
    "            \n",
    "            for fs_name, roc_curves in test_roc_data.items():\n",
    "                if class_label in roc_curves:\n",
    "                    fpr, tpr = roc_curves[class_label]\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    \n",
    "                    display_name = fs_name\n",
    "                    if fs_name.startswith('SHAP_Consensus_'):\n",
    "                        display_name = f\"SHAP_{model_name}\"\n",
    "                    \n",
    "                    ax.plot(fpr, tpr, \n",
    "                        label=f\"{display_name} (AUC={roc_auc:.2f})\",\n",
    "                        linewidth=2,\n",
    "                        color=colors[color_idx % len(colors)])\n",
    "                    color_idx += 1\n",
    "            \n",
    "            ax.plot([0, 1], [0, 1], '--', color='gray', alpha=0.5, linewidth=1)\n",
    "            \n",
    "            ax.set_title(f\"{class_label} – {model_name} – {group_name} (Test Set)\", fontsize=12)\n",
    "            ax.set_xlabel(\"False Positive Rate\")\n",
    "            ax.set_ylabel(\"True Positive Rate\")\n",
    "            ax.legend(loc=\"lower right\", fontsize='small')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        fig.suptitle(f\"{model_name} – {group_name} – Test Set ROC-AUC\", fontsize=16)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        \n",
    "        roc_test_path = output_dir / f\"roc_test_{model_name}.png\"\n",
    "        fig.savefig(roc_test_path, dpi=800, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved test set ROC curves: {roc_test_path}\")\n",
    "\n",
    "def create_test_performance_summary_plot(test_results_df, output_dir):\n",
    "\n",
    "    print(\"Creating test set performance summary plots...\")\n",
    "    \n",
    "    auc_data = test_results_df[test_results_df['Metric'] == 'auc_macro'].copy()\n",
    "    if not auc_data.empty:\n",
    "\n",
    "        pivot_auc = auc_data.pivot_table(\n",
    "            index='Subgroup', \n",
    "            columns=['Model', 'Feature_Set'], \n",
    "            values='Test_Score'\n",
    "        )\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        pivot_auc.plot(kind='bar', ax=ax, width=0.8)\n",
    "        ax.set_title('Test Set AUC Performance by Subgroup and Model', fontsize=16)\n",
    "        ax.set_xlabel('Subgroup', fontsize=12)\n",
    "        ax.set_ylabel('AUC Score', fontsize=12)\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        auc_summary_path = output_dir / \"test_auc_summary_by_subgroup.png\"\n",
    "        plt.savefig(auc_summary_path, dpi=800, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved AUC summary plot: {auc_summary_path}\")\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics = ['auc_macro', 'accuracy', 'f1_macro', 'qwk']\n",
    "    metric_names = ['AUC', 'Accuracy', 'F1 Score', 'Quadratic Weighted Kappa']\n",
    "    \n",
    "    for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "        metric_data = test_results_df[test_results_df['Metric'] == metric]\n",
    "        if not metric_data.empty:\n",
    "            ax = axes[idx]\n",
    "            \n",
    "\n",
    "            feature_sets = metric_data['Feature_Set'].unique()\n",
    "            box_data = []\n",
    "            labels = []\n",
    "            \n",
    "            for fs in feature_sets:\n",
    "                fs_data = metric_data[metric_data['Feature_Set'] == fs]['Test_Score'].values\n",
    "                if len(fs_data) > 0:\n",
    "                    box_data.append(fs_data)\n",
    "\n",
    "                    label = fs.replace('SHAP_Consensus_', 'SHAP_')\n",
    "                    labels.append(label)\n",
    "            \n",
    "            if box_data:\n",
    "                bp = ax.boxplot(box_data, labels=labels, patch_artist=True)\n",
    "                \n",
    "                colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']\n",
    "                for patch, color in zip(bp['boxes'], colors):\n",
    "                    patch.set_facecolor(color)\n",
    "                \n",
    "                ax.set_title(f'{metric_name} Distribution Across Subgroups', fontsize=12)\n",
    "                ax.set_ylabel(metric_name)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                if len(labels) > 3:  \n",
    "                    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.suptitle('Test Set Performance Distribution by Feature Set', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    performance_dist_path = output_dir / \"test_performance_distribution.png\"\n",
    "    plt.savefig(performance_dist_path, dpi=800, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved performance distribution plot: {performance_dist_path}\")\n",
    "\n",
    "# ================== Main Execution ==================\n",
    "def main():\n",
    "    print(\"=\"*80 + \"\\nSUBGROUP ANALYSIS BY AGE & GENDER\\n\" + \"=\"*80)\n",
    "\n",
    "    if not XGB_AVAILABLE and not LGB_AVAILABLE:\n",
    "        print(\"ERROR: Neither XGBoost nor LightGBM is available.\")\n",
    "        return\n",
    "\n",
    "    final_test_results = []  \n",
    "    all_results = []         \n",
    "    all_stability_results = {}  \n",
    "    \n",
    "    data_processor = DataProcessor()\n",
    "    df = data_processor.load_and_preprocess(Config.DATA_PATH)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Execution halted due to missing data file.\")\n",
    "        return\n",
    "\n",
    "    for group_name, group_filter in Config.ANALYSIS_GROUPS.items():\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"Processing subgroup: {group_name}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "\n",
    "        group_df = df[\n",
    "            (df[Config.AGE_COLUMN] >= group_filter['age_min']) &\n",
    "            (df[Config.AGE_COLUMN] <= group_filter['age_max']) &\n",
    "            (df[Config.GENDER_COLUMN] == group_filter['gender'])\n",
    "        ].copy()\n",
    "\n",
    "        n_samples = len(group_df)\n",
    "        if n_samples < Config.MIN_SAMPLES_PER_GROUP:\n",
    "            print(f\"Skipping {group_name} due to insufficient samples ({n_samples}).\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Samples in {group_name}: {n_samples}\")\n",
    "\n",
    "        group_output_dir = Config.OUTPUT_DIR / group_name\n",
    "        group_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X, y, sample_weights = data_processor.get_combined_feature_set(group_df)\n",
    "\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            sw_train, sw_test = None, None\n",
    "            if sample_weights is not None:\n",
    "                sw_train, sw_test = train_test_split(\n",
    "                    sample_weights, test_size=0.2, random_state=42, stratify=y\n",
    "                )\n",
    "            \n",
    "            print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "            \n",
    "            # save test set\n",
    "            test_idx = X_test.index\n",
    "            test_full = df.loc[test_idx].copy()\n",
    "            test_full[\"label_name\"] = test_full[Config.TARGET_COLUMN].map(Config.CLASS_LABELS)\n",
    "            front = [Config.TARGET_COLUMN, \"label_name\", Config.WEIGHT_COLUMN, Config.AGE_COLUMN, Config.GENDER_COLUMN]\n",
    "            front_exist = [c for c in front if c in test_full.columns]\n",
    "            test_full = test_full[front_exist + [c for c in test_full.columns if c not in front_exist]]\n",
    "            test_csv_path = group_output_dir / \"test_set_full.csv\"\n",
    "            test_full.to_csv(test_csv_path, index=False)\n",
    "            print(f\"✅ Full test set saved for {group_name}: {test_csv_path}\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Cannot split {group_name} due to insufficient samples in some classes: {e}\")\n",
    "            continue\n",
    "\n",
    "        pipeline = ModelPipeline()\n",
    "        stability_analyzer = MultiSeedStabilityAnalyzer(data_processor)\n",
    "\n",
    "        group_results = []\n",
    "        group_stability_results = {}\n",
    "        group_test_results = [] \n",
    "\n",
    "        for model_name in ['XGBoost', 'LightGBM']:\n",
    "            available_flag = f\"{model_name.upper()}_AVAILABLE\"\n",
    "            if available_flag == 'XGBOOST_AVAILABLE':\n",
    "                available_flag = 'XGB_AVAILABLE'\n",
    "            elif available_flag == 'LIGHTGBM_AVAILABLE':\n",
    "                available_flag = 'LGB_AVAILABLE'\n",
    "            \n",
    "            if not globals().get(available_flag, False):\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n--- {model_name} for {group_name} ---\")\n",
    "            best_params = pipeline.tune_hyperparameters_once(\n",
    "                model_name, X_train, y_train, sw_train\n",
    "            )\n",
    "\n",
    "            model_results, stability_results = pipeline.evaluate_feature_sets_multiseed(\n",
    "                X_train, y_train, sw_train, model_name, best_params, stability_analyzer, group_output_dir\n",
    "            )\n",
    "\n",
    "            group_results.extend(model_results)\n",
    "            group_stability_results[model_name] = stability_results\n",
    "\n",
    "            test_results = pipeline.evaluate_test_set(\n",
    "                X_train, y_train, X_test, y_test, sw_train, sw_test, \n",
    "                model_name, best_params, stability_results, group_output_dir\n",
    "            )\n",
    "            \n",
    "            for result in test_results:\n",
    "                result['Subgroup'] = group_name\n",
    "            \n",
    "            group_test_results.extend(test_results)\n",
    "\n",
    "        pd.DataFrame(group_results).to_csv(group_output_dir / \"cross_validation_results.csv\", index=False)\n",
    "        if group_test_results:\n",
    "            pd.DataFrame(group_test_results).to_csv(group_output_dir / \"test_results.csv\", index=False)\n",
    "        \n",
    "        print(f\"Saved results for {group_name} in {group_output_dir}\")\n",
    "        \n",
    "        all_results.extend(group_results)\n",
    "        final_test_results.extend(group_test_results)\n",
    "        all_stability_results.update({f\"{group_name}_{k}\": v for k, v in group_stability_results.items()})\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(Config.OUTPUT_DIR / \"all_subgroups_cv_results.csv\", index=False)\n",
    "\n",
    "    if final_test_results:\n",
    "        test_results_df = pd.DataFrame(final_test_results)\n",
    "        test_results_df.to_csv(Config.OUTPUT_DIR / \"all_subgroups_test_results.csv\", index=False)\n",
    "        print(f\"Saved consolidated test results: all_subgroups_test_results.csv\")\n",
    "        \n",
    "        cv_summary = results_df.groupby(['Model', 'Feature_Set', 'Metric'])['Mean'].mean().reset_index()\n",
    "        cv_summary = cv_summary.rename(columns={'Mean': 'CV_Score_Avg'})\n",
    "        \n",
    "        test_summary = test_results_df.groupby(['Model', 'Feature_Set', 'Metric'])['Test_Score'].mean().reset_index()\n",
    "        test_summary = test_summary.rename(columns={'Test_Score': 'Test_Score_Avg'})\n",
    "        \n",
    "        comparison_df = pd.merge(cv_summary, test_summary, on=['Model', 'Feature_Set', 'Metric'], how='outer')\n",
    "        comparison_df['Performance_Gap'] = comparison_df['Test_Score_Avg'] - comparison_df['CV_Score_Avg']\n",
    "        comparison_df['Potential_Overfitting'] = comparison_df['Performance_Gap'] < -0.02  \n",
    "        \n",
    "        comparison_df.to_csv(Config.OUTPUT_DIR / \"cv_vs_test_comparison_by_subgroup.csv\", index=False)\n",
    "        print(f\"Saved performance comparison: cv_vs_test_comparison_by_subgroup.csv\")\n",
    "        create_test_performance_summary_plot(test_results_df, Config.OUTPUT_DIR)\n",
    "    stability_analyzer.stability_results = all_stability_results\n",
    "    \n",
    "    final_summary_data = []\n",
    "    for stability_key, stability in all_stability_results.items():\n",
    "        if 'tooth_level_importance' not in stability: \n",
    "            continue\n",
    "        \n",
    "        parts = stability_key.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            model_name = parts[-1]  \n",
    "            subgroup_name = '_'.join(parts[:-1])  \n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Get the top teeth from the robust ranking DataFrame\n",
    "        tooth_ranking_df = stability['tooth_level_importance']\n",
    "        consensus_teeth = tooth_ranking_df.head(Config.TOP_FEATURES_COUNT)['tooth_fdi'].tolist()\n",
    "        \n",
    "        param_key = f\"{model_name}_Combined\"\n",
    "        params = pipeline.best_params_cache.get(param_key, {})\n",
    "        \n",
    "        test_perf = {}\n",
    "        if final_test_results:\n",
    "            test_df = pd.DataFrame(final_test_results)\n",
    "            combined_test = test_df[\n",
    "                (test_df['Model'] == model_name) & \n",
    "                (test_df['Feature_Set'] == 'Combined') &\n",
    "                (test_df['Subgroup'] == subgroup_name)\n",
    "            ]\n",
    "            for _, row in combined_test.iterrows():\n",
    "                test_perf[f\"Test_{row['Metric']}\"] = f\"{row['Test_Score']:.3f}\"\n",
    "        \n",
    "        shap_test_perf = {}\n",
    "        if final_test_results:\n",
    "            shap_test = test_df[\n",
    "                (test_df['Model'] == model_name) & \n",
    "                (test_df['Feature_Set'] == f'SHAP_Consensus_{model_name}') &\n",
    "                (test_df['Subgroup'] == subgroup_name)\n",
    "            ]\n",
    "            for _, row in shap_test.iterrows():\n",
    "                shap_test_perf[f\"SHAP_Test_{row['Metric']}\"] = f\"{row['Test_Score']:.3f}\"\n",
    "        \n",
    "        summary_row = {\n",
    "            'Subgroup': subgroup_name,  \n",
    "            'Model': model_name,\n",
    "            'Consensus_Teeth_FDI': ', '.join(map(str, sorted(consensus_teeth))) if consensus_teeth else 'None',\n",
    "            'N_Consensus_Teeth': len(consensus_teeth)\n",
    "        }\n",
    "        summary_row.update(params)\n",
    "        summary_row.update(test_perf)\n",
    "        summary_row.update(shap_test_perf)\n",
    "        final_summary_data.append(summary_row)\n",
    "\n",
    "    if final_summary_data:\n",
    "        final_summary_df = pd.DataFrame(final_summary_data).fillna('N/A')\n",
    "        base_cols = ['Subgroup', 'Model', 'Consensus_Teeth_FDI', 'N_Consensus_Teeth']\n",
    "        test_cols = sorted([c for c in final_summary_df.columns if c.startswith('Test_') or c.startswith('SHAP_Test_')])\n",
    "        param_cols = sorted([c for c in final_summary_df.columns if c not in base_cols + test_cols])\n",
    "        final_summary_df = final_summary_df[base_cols + test_cols + param_cols]\n",
    "        final_summary_df.to_csv(Config.OUTPUT_DIR / \"final_consolidated_summary.csv\", index=False)\n",
    "        print(\"Saved: final_consolidated_summary.csv\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\nANALYSIS COMPLETE\\n\" + \"=\"*80)\n",
    "    print(f\"All results, summaries, and plots have been saved to: {Config.OUTPUT_DIR}\")\n",
    "    print(\"\\nKey outputs:\")\n",
    "    print(\"- cross_validation_results.csv: Multi-seed cross-validation performance (on training set)\")\n",
    "    print(\"- final_test_results.csv: Unbiased test set performance\")  \n",
    "    print(\"- cv_vs_test_comparison.csv: Cross-validation vs test performance comparison\")\n",
    "    print(\"- final_consolidated_summary.csv: Complete summary with test scores\")\n",
    "    print(\"\\nData usage summary:\")\n",
    "    print(\"- Training set (80%): Used for hyperparameter tuning, cross-validation, and SHAP analysis\")\n",
    "    print(\"- Test set (20%): Used only for final unbiased evaluation\")\n",
    "\n",
    "    all_test_full = []\n",
    "    for group_name, group_filter in Config.ANALYSIS_GROUPS.items():\n",
    "        test_path = Config.OUTPUT_DIR / group_name / \"test_set_full.csv\"\n",
    "        if test_path.exists():\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            test_df[\"Subgroup\"] = group_name\n",
    "            all_test_full.append(test_df)\n",
    "\n",
    "    if all_test_full:\n",
    "        pd.concat(all_test_full).to_csv(Config.OUTPUT_DIR / \"all_subgroups_test_set_full.csv\", index=False)\n",
    "        print(\"✅ Saved consolidated full test set: all_subgroups_test_set_full.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
